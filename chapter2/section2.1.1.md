## 二进制理论

### 序言

二进制是一个以2为基数的记数系统，其使用两个相互排斥的状态来表达信息。一个二进制数由被称为***比特***（译者：亦称***二进制位***）的元素构成，每个比特可以处于两个可能状态中的一个。通常，我们使用数字1和0来描述他们。我们谈起他们的时候也可以是真和假。对电子技术而言，这两种状态可以用高电压或低电压表示或某种形式的开关被打开或闭合。

我们使用与构造传统10基数系统数字相同的方法来构建二进制数。但是，我们不使用个位，十位，百位（等等），而是使用个位，二位，四位和八位，等等，如下面所说明的。

### 表2.1 二进制

| 2... | $$ 2^6 $$ | $$ 2^5 $$ | $$ 2^4 $$ | $$ 2^3 $$ | $$ 2^2 $$ | $$ 2^1 $$ | $$ 2^0 $$ |
| :--: | :-------: | :-------: | :-------: | :-------: | :-------: | :-------: | :-------: |
| ...  |    64     |    32     |    16     |     8     |     4     |     2     |     1     |

例如，在10进制中表示数字203，我们知道把3放在个位，0放在十位，2放在百位。这从下面表中的指数能表现出来。

### 表2.2 以10为基数的203

| $$10^2$$ | $$10^1$$ | $$10^0$$ |
| :------: | :------: | :------: |
|    2     |    0     |    3     |

换句话说，$$2\times10^2+3\times10^0=200+3=203$$。为了在二进制中表示一样的东西，有如下表。

### 表2.3 以2为基数的203

| $$2^7$$ | $$2^6$$ | $$2^5$$ | $$2^4$$ | $$2^3$$ | $$2^2$$ | $$2^1$$ | $$2^0$$ |
| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |
|    1    |    1    |    0    |    0    |    1    |    0    |    1    |    1    |

这等于$$2^7+2^6+2^3+2^1+2^0=128+64+8+2+1=203$$。

### 计算机科学的基础

你也许想问一个简单的数字怎么就是所有这些计算机能做到的神奇事务的基础。不管你信不信，它就是！在你计算机的处理器中有一个复杂但实际上有限的指令集，它可以对数值进行加法或乘法等操作。大体上，每一个这些指令都被分配一个数因此整个程序（加法，乘法除法等等）被表示成一个数字流。例如，如果处理器知道操作2是加法，则252可以表示『5加上2然后存储输出到某个地方』。当然其实真实情况要复杂的多（见[第三章，计算机架构](/chapter3)），但是，总而言之，计算机就是这么个东西。

在打孔卡时代，人们可以通过查看卡片上的洞眼来看到组成程序流的0和1。当然后来通过小磁性颗粒的磁极储存更快（磁带，磁碟），直到今天，我们可以在将不可想象的海量数据装进口袋。

将这些数字翻译成对人类有用的东西使得计算机如此有用。举个例子，显示器由百万级互不相关的***像素点***构成，每一个像素对人眼来说都难以辨认但是组合起来能形成一个完整的图像。通常每个像素由某些红色，绿色和蓝色部件构成它的显示色彩。当然，这些值可以被一个数来表示，当然，这可以用二进制表示！因此任何图像都可以被拆分成上百万个单独的点，每个点可以用一个三值***元组***来表示像素红色，绿色和蓝色的值。因此给定一长串这样的数，以正确的格式，计算机中的视频硬件将会把这些数字转换成电子信号来打开或关闭特定的像素点从而显示图像。

随着你继续读下去，我们将从这一构成要素搭建起整个现代计算机环境；如果你想的话，***自下而上***。

### 二进制位与字节

通过上面的探讨，我们基本上可以用一个数表示任何东西，这个数可以转换成二进制且在计算机上进行操作。例如，表示字母表上的所有字母我们需要至少足够的不同组合来表示小写字母，大写字母，数字和标点符号，再加上一些其他东西。把这些都加起来意味着我们大概需要大约80个不同组合。

如果我们有两个比特，我们可以表示4种可能的独特组合（00 01 10 11）。如果我们由三个比特，我们可以表示8种不同的组合。一般而言，使用$$n$$个二进制位我们可以表示$$2^n$$种不同组合。

8个二进制位可以给我们提供$$2^8=256$$种不同的表示，远远超出了我们字母表的组合。我们把一组8个二进制位称为一个***字节***。猜猜一个C语言char类型变量有多大？一个字节。

### ASCII

考虑到一个字节可以表示0到255的任何数值，任何人都能武断地将字符与数字建立映射。比如说，一个视频卡制造商可以决定1代表A，所以当数值1被发送到视频卡时它将在屏幕上显示一个大写『A』。一个打印机制造商或许由于某些使人费解的原因决定让1表示小写『z』,意味着显示和打印同样的东西时需要那些复杂的转换。

为了避免这些发生，***美国信息交换标准代码***或***ASCII***诞生了。这是一个7比特位的代码，意味着有$$2^7$$或128个可用码。

代码区域被划分成了2个主要的部分；可打印部分以及不可打印部分。可打印部分是类如字符（大写和小写），数字以及标点符号的这种东西。不可打印部分是用于控制，以及回车换行，敲响终端铃或者特别的***NULL***码，其表示什么也没有。

127个不同的字符对与美式英语来说足够了，但是当人们想要表示他们自己语言中常见字符的时候就变得非常局限，尤其是亚洲语言，其可以有成千上万种不同的字符。

为了缓解这种症状，现代系统都从ASCII转向***Unicode***，其最多可以使用4个字节来表达一个字符，提供了更加多的空间！

#### 奇偶校验

ASCII，作为一个只有7比特的码，字节中留下了一个多余的二进制位。这可以被用来实现***奇偶校验***，这是一个简单的错误检验形式。想象一个计算机使用打孔卡来输入，有洞的地方表示1没洞的地方表示0。任何覆盖住了一个洞的意外都会导致数据被错误读取，导致不可预计行为。

奇偶校验为字节中的二进制位提供了简单的检查确保他么被正确地读取。通过使用多余的一个二进制位作为***奇偶校验位***可以实现奇校验或偶校验。

在奇校验中，如果7比特信息中1的数量是奇数，则奇偶校验位被设置1，不然不被设置。偶校验是相反的；如果1的数量是偶数则校验位设为1。

这样一来，一个二进制位的翻转会导致一个奇偶校验错误，这是可以被察觉的。

XXX更多有关错误修正。（迷...这是什么鬼）

XXXmore about error correcting

#### 16,32和64位计算机

数字不适合字节；希望你银行余额要比放的进字节的范围要大！现代计算机体系结构少是32位。这意味着它们可以同时对内存处理，读取或写入4个字节。我们把4个字节称为一个***字***；这与语言中字母组成句子中的单词是类似的，除了计算机科学中每个字的大小是相同的。C语言int类型变量的大小是32位。现计算机架构是64位，大小是8字节处理器的两倍。

#### 千（K），兆（M）和京（G）字节

计算机和许多字节打交道；这就是使他如此强大的原因！我们需要一种论述大量字节的方法，一个自然而然的方法就是像其他许多科学领域一样，使用『国际单位制』（SI）前缀。比如，千（Kilo）代表$$10^3$$即1000个单位，就像1千克有1000克。

1000是十进制里一个好看的概数，但是在二进制中它是1111101000，并不是一个准确的『概数』。不管怎样，1024（或$$2^10$$）是一个概数——（10000000000）——而且恰好与10进制的『千』非常接近（1000对比1024）。因此1024字节自然而然就被称为***千字节（kilobyte=KB）***。下一个SI单位是『兆』（mega）,指$$10^6$$而且这个前缀以$$10^3$$连续上升。（与写大数字时通常3位一组划分相对应）。这样，$$2^20$$与SI十进制兆的定义又一次相近；1048576对应1000000。通过10的幂运算增加二进制的单位基本上维持了与十进制的SI数值的相近，虽然每次增加导致与基本SI含义的差距慢慢变大。综上SI十进制单位『很接近』而且使用在二进制数值变得越来越常用。

### 表2.4 涉及字节的以2为基数以及以10为基数的系数

| 名称         | 2基数系数    | 字节                        | 相近的10基数系数 | 10基数字节                    |
| ---------- | -------- | ------------------------- | --------- | ------------------------- |
| 1 Kilobyte | $$2^10$$ | 1,024                     | $$10^3$$  | 1,000                     |
| 1 Megabyte | $$2^20$$ | 1,048,576                 | $$10^6$$  | 1,000,000                 |
| 1 Gigabyte | $$2^30$$ | 1,073,741,824             | $$10^9$$  | 1,000,000,000             |
| 1 Terabyte | $$2^40$$ | 1,099,511,627,776         | $$10^12$$ | 1,000,000,000,000         |
| 1 Petabyte | $$2^50$$ | 1,125,899,906,842,624     | $$10^15$$ | 1,000,000,000,000,000     |
| 1 Exabyte  | $$2^60$$ | 1,152,921,504,606,846,976 | $$10^18$$ | 1,000,000,000,000,000,000 |

   基数为2与基数为10的SI单位间的比较。



在帮助快速在『人类』度量与二进制位的度量之间建立关联方面，把基数为2的因子提交到内存是非常有成效的。例如，我们可以快速计算出32位计算机可以通过记录$$2^2(4)+ 2^30$$的重组来寻址高达4千兆字节（4GB）的内存。相似地64位则可以寻址16艾字节（Exabyte）（$$2^4+2^60$$）；你也许有兴趣搞明白这是个多大的数字。想感受下这个数字有多大，计算下如果一秒增加一次数到$$2^64$$会花多久。

#### 千，兆，吉比特

撇开二进制与十进制间SI单位的超负荷导致的困惑，比起字节，二进制位更常用于描述容量。通常当我们谈及网络或储存设备时这会发生；你也许会注意到你的ADSL宽带速率被描述成类似于1500千比特每秒(Kb/s）的样子。计算很简单；乘以1000（因为kilo），除以8转换为字节，然后除以1024得到千字节（所以1500kilobits/s=183kilobytes/s)

SI标准化机构认识到了这些双重使用，然后为二进制使用制定了专有的前缀。在其标准下1024字节的表示是是***kibibyte***，是kilo binary byte的缩写（缩写为KiB）。其他前缀也是类似的（比如Mebibyte,MiB）。庞大的传统阻止了这些术语的使用，但是也许你会在某些文献中看到它们。

#### 转换

进制之间最简单的转换方法就是使用计算机，毕竟，这是他们擅长的东西。不管咋样，知道怎么徒手转换总是有用的。

进制间最简单的转换方法就是***重复除法***。转换中，反复用商除以基数，直到商为0，每一步都记下余数。然后逆序写下余数，从最底部开始每一次都追加到右边。如下列所展示；因为我们要转换成二进制所以基数为2。

### 表2.5 将203转为二进制

|                     | 商    | 余数   |      |
| ------------------- | ---- | ---- | ---- |
| $$203_{10}\div2=$$  | 101  | 1    |      |
| $$101_{10}\div2 =$$ | 50   | 1    | ↑    |
| $$50_{10}\div2=$$   | 25   | 0    | ↑    |
| $$25_{10}\div2=$$   | 12   | 1    | ↑    |
| $$12_{10}\div2=$$   | 6    | 0    | ↑    |
| $$6_{10}\div2=$$    | 3    | 0    | ↑    |
| $$3_{10}\div2=$$    | 1    | 1    | ↑    |
| $$1_{10}\div2=$$    | 0    | 1    | ↑    |

每一次从下往上读，然后写在前一位的右侧，得到11001011，通过之前的例子我们可以看出这是203。

### 逻辑运算

George Boole是一个数学家，他开拓了数学的一块领域——***逻辑代数***（布尔代数）。他在19世纪中期进行的这项探索，将会是所有计算机科学的基础。逻辑代数是一个范围很广的话题，我们在这里展现的只是冰山一角，使得你能够顺利启程。

逻辑运算就是简单的得到一个输入后按一个规则产生一个特定的输出。举个例子，最简单的逻辑运算，***非***运算就是简单地逆转输入操作的数值。其他运算通常有两个输入，然后产生一个输出。

计算机科学中使用的基本逻辑运算很容易记忆，如下面所列出来的这些。我们用下面的***真值表***来演绎他们；他们单纯地展示出所有可能的输入和输出。在二进制中**真**对应于**1**。

#### 非

通常用***!***表示，非操作简单的将数值反转，0变成1，1变成0。

### 表 2.6 非运算的真值表

| Input | Output |
| ----- | ------ |
| `1`   | `0`    |
| `0`   | `1`    |

#### 与

想要记忆与运算如何工作的，把它想成『如果一个输入和另一个输入为真，则结果为真』

### 表2.7 与运算的真值表

| Input 1 | Input 2 | Output |
| ------- | ------- | ------ |
| `0`     | `0`     | `0`    |
| `1`     | `0`     | `0`    |
| `0`     | `1`     | `0`    |
| `1`     | `1`     | `1`    |

#### 或

要去记忆或运算是如何工作的，把它记成『如果一个输入或另一个输入为真，则结果为真』

### 表2.8 或运算真值表

| Input 1 | Input 2 | Output |
| ------- | ------- | ------ |
| `0`     | `0`     | `0`    |
| `1`     | `0`     | `1`    |
| `0`     | `1`     | `1`    |
| `1`     | `1`     | `1`    |

#### 异或

异或，写作xor，是或运算的一个特殊情况，当如果一个输入为真，且***仅有***一个输入为真，则输出为真。这个运算可以惊人地耍许多有趣的把戏，但你在内核中不会经常见到它。

### 表2.9 异或运算的真值表

| Input 1 | Input 2 | Output |
| ------- | ------- | ------ |
| `0`     | `0`     | `0`    |
| `1`     | `0`     | `1`    |
| `0`     | `1`     | `1`    |
| `1`     | `1`     | `0`    |

### 计算机是如何使用逻辑运算的

不管你信不信，基本上你计算机干的所有事都会回到上面这些运算。比如，半加器是一种由逻辑运算制造成的电路，它能把比特加在一起（它被称为半加器因为它不能处理进位）。把更多半加器放在一起，你就能开始构建某个可以相加长二进制数的东西。再加上点存储器，你就有了一台计算机。

电子工程上来说，逻辑运算是在由晶体管组成的门电路中实现的。这就是为什么你会听到什么晶体管计数或者类似摩尔定律的东西。越多的晶体管，越多门电路，你能把越多东西加在一起。创造一台现代计算机，会用到大量的晶体管，大量的门电路。一些最新的安藤处理器已经有了大约46千万个晶体管。

### 在C语言中操作二进制

在C语言里，对这些以上运算我们都有一个直接的接口。下面的表格表述了这些运算符。

### 表2.10 C语言中的逻辑运算



| 运算    | C语言中的使用             |
| ----- | ------------------- |
| `not` | `!`                 |
| `and` | `&`                 |
| `or`  | <code>&#124;</code> |
| `xor` | `^`                 |

我们在变量上使用这些运算来改变变量内部的比特。在我们看到对这个的举例之前，我们先要改道来探讨下十六进制标记。